<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>St. Jude Rust Labs RFCs</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-bfef3c60.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d406e065.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>‚Üê</kbd> or <kbd>‚Üí</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">St. Jude Rust Labs RFCs</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <img style="margin: 0px" alt="Repository Header Image" src="resources/repo-header.png" />
<h1 id="requests-for-comments-for-the-st-jude-rust-labs-project"><a class="header" href="#requests-for-comments-for-the-st-jude-rust-labs-project">Requests For Comments for the St. Jude Rust Labs project</a></h1>
<p>This repo structure and contents are <del>stolen</del> borrowed from the <a href="https://github.com/stjudecloud/rfcs">St. Jude Cloud Team‚Äôs <code>rfcs</code> repo</a>.</p>
<p>These RFCs are meant to act as a public archive of our design intentions and to facilitate conversation both amongst the internal team and any external parties who may wish to help shape the future of the Sprocket project. Notably, these documents are not authoritative in any fashion. They are a snapshot of design goals; some details are important to hash out ahead of time, but practical experience with an implementation or changing understanding of a problem space as learned through use of a feature will likely cause deviations from the initial plans as laid out in these RFCs.</p>
<p>We will not make an effort to backport changes to these documents if we feel the conversations have run their course.</p>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<pre><code class="language-sh">cargo install mdbook
</code></pre>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<pre><code class="language-sh">mdbook build
python3 -m http.server -d book
# visit the rendered version in your browser at http://localhost:8000.
</code></pre>
<h2 id="-license-and-legal"><a class="header" href="#-license-and-legal">üìù License and Legal</a></h2>
<p>This project is licensed as either <a href="https://github.com/stjude-rust-labs/rfcs/blob/main/LICENSE-APACHE">Apache 2.0</a> or
<a href="https://github.com/stjude-rust-labs/rfcs/blob/main/LICENSE-MIT">MIT</a> at your discretion. Additionally, please see <a href="https://github.com/stjude-rust-labs#disclaimer">the
disclaimer</a> that applies to all
crates and command line tools made available by St. Jude Rust Labs.</p>
<p>Copyright ¬© 2023-Present <a href="https://github.com/stjude">St. Jude Children‚Äôs Research Hospital</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<ul>
<li>Feature Name: sprocket-test</li>
<li>Start Date: 2025-08</li>
</ul>
<h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p>The Sprocket test framework enables WDL authors to easily and comprehensively validate their WDL tasks and workflows by defining lightweight unit tests that can run in CI environments. This framework is intended to be intuitive and concise.</p>
<p>This framework can maximize test depth without adding boilerplate by allowing users to define ‚Äútest matrices‚Äù, where each WDL task or workflow is run with permutations of the provided inputs.</p>
<h1 id="motivation"><a class="header" href="#motivation">Motivation</a></h1>
<p>Comprehensive unit testing is a key component of modern software development. Any serious project should endeavor to have a suite of tests that ensure code correctness. These tests should be lightweight enough to run during continuous integration on every set of committed changes.</p>
<p>n.b.: This RFC is primarily focused on CI unit testing, but enabling larger scale ‚Äúend-to-end‚Äù testing is something that should be kept in mind during this design process. That said, I‚Äôm of the opinion these are separate enough use cases that they can be approached with differing APIs, and thus decisions made here should not impact any future E2E testing API too heavily.</p>
<h1 id="guide-level-explanation"><a class="header" href="#guide-level-explanation">Guide-level explanation</a></h1>
<p>The Sprocket test framework is primarily specified in TOML, which is expected to be in a file of the same basename as the WDL being tested, but with the <code>.wdl</code> extension replaced by <code>.toml</code>. <code>sprocket test</code> does not require any special syntax or modification of actual WDL files, and any WDL workspace can begin writing tests without needing to refactor their WDL documents. Following this pattern frees the TOML from having to contain any information about <em>where</em> to find the entrypoint of each test. For example, all the test entrypoints (tasks or workflows) in <code>data_structures/flag_filter.toml</code> are expected to be defined in the WDL located at <code>data_structures/flag_filter.wdl</code>.</p>
<p>Any task or workflow defined in <code>data_structures/flag_filter.wdl</code> can have any number of tests associated with it in <code>data_structures/flag_filter.toml</code>. To write a set of tests for a task <code>validate_string_is_12bit_int</code> in <code>flag_filter.wdl</code>, the user will define an array of tables in their TOML with the header <code>[[validate_string_is_12bit_int]]</code>. Multiple tests can be written for the <code>validate_string_is_12bit_int</code> task by repeating the <code>[[validate_string_is_12bit_int]]</code> header. The <code>validate_flag_filter</code> workflow can be tested the same way, by defining a TOML array of tables headered with <code>[[validate_flag_filter]]</code>. These TOML headers must match an entrypoint in the corresponding WDL file. Under each of these headers will be a TOML table with all the required information for a single test.</p>
<p>An example TOML for specifying a suite of tests for <a href="https://github.com/stjudecloud/workflows/blob/main/data_structures/flag_filter.wdl">the <code>flag_filter.wdl</code> document in the <code>workflows</code> repo</a> would look like:</p>
<pre><code class="language-toml">[[validate_string_is_12bit_int]]
name = "decimal_passes" # each test must have a unique identifier
[validate_string_is_12bit_int.inputs]
number = "5"
# without any assertions explicitly configured, Sprocket will consider the task executing with a 0 exit code to be a "pass" and any non-zero exit code as a "fail"

[[validate_string_is_12bit_int]]
name = "hexadecimal_passes"
[validate_string_is_12bit_int.inputs]
number = "0x900"
[validate_string_is_12bit_int.assertions]
stdout.contains = "Input number (0x900) is valid" # builtin assertion for checking STDOUT logs

[[validate_string_is_12bit_int]]
name = "too_big_hexadecimal_fails"
[validate_string_is_12bit_int.inputs]
number = "0x1000"
[validate_string_is_12bit_int.assertions]
exit_code = 42 # the task should fail for this test
stderr.contains = "Input number (0x1000) is invalid" # similar to the stdout assertion

[[validate_string_is_12bit_int]]
name = "too_big_decimal_fails"
[validate_string_is_12bit_int.inputs]
number = "4096"
[validate_string_is_12bit_int.assertions]
exit_code = 42
stderr.contains = [
    "Input number (4096) interpreted as decimal",
    "But number must be less than 4096!",
] # `contains` assertion can also be an array of strings

[[validate_flag_filter]] # a workflow test
name = "valid_FlagFilter_passes"
[validate_flag_filter.inputs.flags]
include_if_all = "3" # decimal
exclude_if_any = "0xF04" # hexadecimal
include_if_any = "03" # octal
exclude_if_all = "4095" # decimal

[[validate_flag_filter]]
name = "invalid_FlagFilter_fails"
[validate_flag_filter.inputs.flags]
include_if_all = "" # empty string
exclude_if_any = "this is not a number"
include_if_any = "000000000011" # binary interpreted as octal. Too many digits for octal
exclude_if_all = "4095" # this is fine
[validate_flag_filter.assertions]
should_fail = true
</code></pre>
<p>Hopefully, everything in the above TOML is easily enough grokked that I won‚Äôt spend time going through the specifics in much detail. The <code>flag_filter.wdl</code> WDL document contains a task and a workflow, both with minimal inputs and no outputs, making the tests fairly straightforward. One of Sprocket‚Äôs guiding principles is to only introduce complexity where it‚Äôs warranted, and I hope that this example demonstrates a case where complexity is <em>not</em> warranted. Next, we will be discussing features intended for allowing test cases that are more complex, but the end API exposed to users (the focus of this document) still aims to maintain simplicity and intuitiveness.</p>
<h2 id="test-data"><a class="header" href="#test-data">Test Data</a></h2>
<p>Most WDL tasks and workflows have <code>File</code> type inputs and outputs, so there should be an easy way to incorporate test files into the framework. This can be accomplished with a <code>tests/fixtures/</code> directory in the root of the workspace which can be referred to from any TOML test. If the string <code>$FIXTURES</code> is found within a TOML string value within the <code>inputs</code> table, the correct path to the <code>fixtures</code> directory will be dynamically inserted at test run time. This avoids having to track relative paths from TOML that may be arbitrarily nested in relation to test data. For example, let‚Äôs assume there are <code>test.bam</code>, <code>test.bam.bai</code>, and <code>reference.fa.gz</code> files located within the <code>tests/fixtures/</code> directory; the following TOML <code>inputs</code> table could be used regardless of where that actual <code>.toml</code> file resides within the WDL workspace:</p>
<pre><code class="language-toml">bam = "$FIXTURES/test.bam"
bam_index = "$FIXTURES/test.bam.bai"
reference_fasta = "$FIXTURES/reference.fa.gz"
</code></pre>
<h2 id="builtin-assertions"><a class="header" href="#builtin-assertions">Builtin Assertions</a></h2>
<p>Sprocket will support a variety of common test conditions. In this document so far, we‚Äôve seen a few of the most straightforward conditions already in the <code>assertions</code> table of the earlier TOML example (<code>exit_code</code>, <code>stdout.contains</code>, <code>stderr.contains</code> and <code>should_fail</code>). For the initial release of <code>sprocket test</code>, these builtin assertions will probably remain as a rather small and tailored set, but the implementation should make extending this set in subsequent releases simple and non-breaking. Adding new builtin assertions could be a recommended starting point for new contributors, similar to how new lint rules are fairly straightforward to add.</p>
<p>Some assertions that might exist at initial release:</p>
<ul>
<li><code>exit_code = &lt;int&gt;</code> (should an array of ints be supported?)</li>
<li><code>should_fail = &lt;bool&gt;</code>: only available for workflow tests! Task tests should instead specify an <code>exit_code</code></li>
<li><code>stdout</code>: will be a TOML table with a sub-tests related to checking a tasks STDOUT log (<em>not available</em> for workflow tests)
<ul>
<li><code>contains = &lt;string | array of strings&gt;</code>: strings are REs</li>
<li><code>not_contains = &lt;string | array of strings&gt;</code>: strings are REs</li>
</ul>
</li>
<li><code>stderr</code>: functionally equivalent to the <code>stdout</code> tests, assertions but runs on the STDERR log instead</li>
<li><code>outputs</code>: a TOML table populated with task or workflow output identifiers. The specific assertions available will depend on the WDL type of the specified output
<ul>
<li><code>&lt;WDL Boolean&gt; = &lt;true|false&gt;</code></li>
<li><code>&lt;WDL Int&gt; = &lt;TOML int&gt;</code></li>
<li><code>&lt;WDL Float&gt; = &lt;TOML float&gt;</code></li>
<li><code>&lt;WDL String&gt;</code>
<ul>
<li><code>contains = &lt;string | array of strings&gt;</code>: strings are REs</li>
<li><code>not_contains = &lt;string | array of strings&gt;</code>: strings are REs</li>
<li><code>equals = &lt;string&gt;</code> an exact match RE</li>
</ul>
</li>
<li><code>&lt;WDL File&gt;</code>
<ul>
<li><code>name = &lt;string&gt;</code>: glob pattern that should match</li>
<li><code>md5 = &lt;string&gt;</code>: md5sum that the file should have</li>
<li><code>blake3 = &lt;string&gt;</code>: blake3 hash that the file should have</li>
<li><code>sha256 = &lt;string&gt;</code>: sha256 hash that the file should have</li>
<li><code>contains = &lt;string | array of strings&gt;</code>: REs to expect within the file contents</li>
<li><code>not_contains = &lt;string | array of strings&gt;</code>: inverse of <code>contains</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The above is probably (about) sufficient for an initial release. Thoughts about future assertions that could exist will be discussed in the <a href="#future-possibilities">‚ÄúFuture possibilities‚Äù</a> section.</p>
<h2 id="custom-assertions"><a class="header" href="#custom-assertions">Custom Assertions</a></h2>
<p>While the builtin assertions should try and address many common use cases, users need a way to test for things outside the scope of the builtins (especially at launch, when the builtins will be minimal). There needs to be a way for users to execute arbitrary code on the outputs of a task or workflow for validation. This will be exposed via the <code>assertions.custom</code> test, which will accept a name or array of names of user supplied executables (most commonly shell or Python scripts) which are expected to be found in a <code>tests/custom/</code> directory. These executables will be invoked with a positional argument which is a path to the task or workflow‚Äôs <code>outputs.json</code>. Users will be responsible for parsing that JSON and performing any validation they desire. So long as the invoked executable exits with a code of zero, the test will be considered as passed.</p>
<p>For further discussion of why this design was chosen, see the <a href="#rationale-and-alternatives">rationale section</a>. The path to an <code>outputs.json</code> is required for this to be usable, but we could consider other paths or information which may be valuable in a test context which we could expose via other arguments or environment variables.</p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p><code>tools/picard.toml</code></p>
<pre><code class="language-toml">[[merge_sam_files]]
name = "Merge works"
[merge_sam_files.inputs]
bams = [
    "$FIXTURES/test1.bam",
    "$FIXTURES/test2.bam",
]
prefix = "test.merged"
[merge_sam_files.assertions]
custom = "quickcheck.sh"
</code></pre>
<p>Sprocket will look for an executable file named <code>quickcheck.sh</code> in the <code>tests/custom/</code> directory. That file could contain any arbitrary code, such as:</p>
<pre><code class="language-bash">#!/bin/bash

set -euo pipefail

out_json=$1

out_bam=$(jq -r .bam "$out_json")

samtools quickcheck "$out_bam"
</code></pre>
<h2 id="test-matrices"><a class="header" href="#test-matrices">Test Matrices</a></h2>
<p>Often, it makes sense to validate that a variety of inputs result in the same test result. While the TOML definitions shared so far are relatively concise, repeating the same test conditions for many different inputs can get repetitive and the act of writing redundant boilerplate can discourage testing best practices. Sprocket offers a ‚Äúshortcut‚Äù for avoiding this boilerplate, by defining test matrices. These test matrices can be a way to reach comprehensive test depth with minimal boilerplate. A test matrix is created by defining a <code>matrix</code> TOML array of tables for a set of test inputs. Each permutation of the ‚Äúinput vectors‚Äù will be run, which can be leveraged to test many conditions with a single test definition. Sprocket will evaluate the Cartesian product of the tables in the <code>matrix</code> array and run each combination of input values.</p>
<p>Below, you will find an example for a <code>bam_to_fastq</code> task defines <code>3*2*2*2*2*2*1 = 96</code> different permutations of the task inputs which should each be executed by Sprocket using only ~30 lines of TOML.</p>
<pre><code class="language-toml">[[bam_to_fastq]]
name = "kitchen_sink"
[[bam_to_fastq.matrix]]
bam = [
    "$FIXTURES/test1.bam",
    "$FIXTURES/test2.bam",
    "$FIXTURES/test3.bam",
]
bam_index = [
    "$FIXTURES/test1.bam.bai",
    "$FIXTURES/test2.bam.bai",
    "$FIXTURES/test3.bam.bai",
]
[[bam_to_fastq.matrix]]
bitwise_filter = [
    { include_if_all = "0x0", exclude_if_any = "0x900", include_if_any = "0x0", exclude_if_all = "0x0" },
    { include_if_all = "00", exclude_if_any = "0x904", include_if_any = "3", exclude_if_all = "0" },
]
[[bam_to_fastq.matrix]]
paired_end = [true, false]
[[bam_to_fastq.matrix]]
retain_collated_bam = [true, false]
[[bam_to_fastq.matrix]]
append_read_number = [true, false]
[[bam_to_fastq.matrix]]
output_singletons = [true, false]
[[bam_to_fastq.matrix]]
prefix = ["kitchen_sink_test"] # the `prefix` input will be shared by _all_ permutations of the test matrix
# this test is to ensure all the options (and combinations thereof) are valid
# so no assertions beyond a `0` exit code are needed here
</code></pre>
<p>This is perhaps an extreme test case, but it was contrived as a stress test of sorts for the matrix design. This specific case may be too intense to run in a CI environment, but should demonstrate the power of test matrices in aiding comprehensive testing without undue boilerplate.</p>
<p>(Notably, the actual <code>bam_to_fastq</code> task in <code>samtools.wdl</code> (<a href="https://github.com/stjudecloud/workflows/blob/main/tools/samtools.wdl#L763">here</a>) does not have a <code>bam_index</code> input, but that was added to this example for illustrative purposes)</p>
<p>REVIEWERS: I can write more examples of ‚Äúreal‚Äù TOML test files, as presumably we will be switching the <code>workflows</code> repo to this framework, in which case any tests written as examples here can hopefully just be re-used with minimal modification for the production tests we want. So don‚Äôt be afraid to ask for more examples! I just didn‚Äôt want to overload this document ;D</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>All of the expected paths, <code>tests/fixtures/</code> and <code>tests/custom/</code>, will be configurable. <code>tests/</code> conflicts with <code>pytest-workflow</code>, so users may want to rename the default directories to something like <code>sprocket-tests/</code>.</p>
<h2 id="test-filtering"><a class="header" href="#test-filtering">Test Filtering</a></h2>
<p>Users will be able to annotate each test with arbitrary tags which will allow them to run subsets of the entire test suite. They will also be able to run the tests in a specific file, as opposed to the default <code>sprocket test</code> behavior which will be to recurse the working directory and run all found tests. This will facilitate a variety of applications, most notably restricting the run to only what the developer knows has changed and parallelizing CI runs.</p>
<p>We may also want to give some tags special meaning: it is common to annotate ‚Äúslow‚Äù tests and to exclude them from runs by default and we may want to make reduce friction in configuring that case.</p>
<h1 id="reference-level-explanation"><a class="header" href="#reference-level-explanation">Reference-level explanation</a></h1>
<p>REVIEWERS: is this section needed?</p>
<hr>
<p>This is the technical portion of the RFC. Explain the design in sufficient detail that:</p>
<ul>
<li>Its interaction with other features is clear.</li>
<li>It is reasonably clear how the feature would be implemented.</li>
<li>Corner cases are dissected by example.</li>
</ul>
<p>The section should return to the examples given in the previous section, and explain more fully how the detailed proposal makes those examples work.</p>
<h1 id="drawbacks"><a class="header" href="#drawbacks">Drawbacks</a></h1>
<p>Q: What are reasons we should we <em>not</em> do this?</p>
<p>A: <em>none!</em> This is a great idea!</p>
<p>To be serious, <code>pytest-workflow</code> seems to be the best test framework for WDL that I‚Äôve been able to find as a WDL author, and as a user of that framework, I think WDL could use something more tailored. I will elaborate further in <a href="#prior-art">Prior Art</a>.</p>
<h1 id="rationale-and-alternatives"><a class="header" href="#rationale-and-alternatives">Rationale and alternatives</a></h1>
<p>REVIEWERS: I‚Äôve thought through quite a wide variety of implementations that have not made it into writing, and I‚Äôm not sure how valuable my musings on alternatives I <em>didn‚Äôt like</em> are. I can expand on this section if it would be informative.</p>
<h2 id="custom-assertions-rationale"><a class="header" href="#custom-assertions-rationale">Custom Assertions Rationale</a></h2>
<p>The custom assertion design is meant to maximize flexibility without adding implementation complexity. The implementation proposed couldn‚Äôt be much simpler: simply invoke an arbitrary executable with a single positional argument, expect an exit code of zero and anything else is a failed test.</p>
<p>This child process will inherit the parent process‚Äôs environment, and it will ultimately be up to test authors for ensuring their test environment and dependencies are correct. This may lead to debugging difficulties, and Sprocket will be able to offer very little help with what‚Äôs going on (aside from forwarding the STDOUT and STDERR streams).</p>
<p>This is a large drawback of the design, but I believe the flexibility offered here is worth those pains. Users can drop shell scripts, Python scripts, bespoke Rust binaries, or anything else using any framework they want, so long as it has a <code>+x</code> bit and can process a positional argument.</p>
<h1 id="prior-art"><a class="header" href="#prior-art">Prior art</a></h1>
<p>This document has been largely informed by my experience as a WDL author and maintainer of the <a href="https://github.com/stjudecloud/workflows">stjudecloud/workflows</a> repository. The CI of that repo uses <code>pytest-workflow</code>.</p>
<h2 id="pytest-workflow"><a class="header" href="#pytest-workflow"><code>pytest-workflow</code></a></h2>
<p><a href="https://github.com/lumc/pytest-workflow/"><code>pytest-workflow</code></a> has been a great stop-gap tool for us. It is a generalized test framework not specific to WDL, which is ultimately what makes it unwieldly for our use cases. The generality of <code>pytest-workflow</code> necessitates <em>a lot</em> of boilerplate in our tests, and was proving a disincentive to writing comprehensive tests. Tests were a pain to write, as a lot of redundant text had to be written for hooking up inputs and outputs in a way that both <code>pytest-workflow</code> and a WDL engine could work with.</p>
<p>The WDL community should have a better solution than a generic test framework.</p>
<p>That said, if you are familiar with pytest-workflow, you will likely see some similarities between it and my proposal. I‚Äôve leaned on the existing designs used in pytest-workflow, and made them more specific and ergonomic for WDL. There are 3 primary ways this RFC distinguishes itself from pytest-workflow:</p>
<ol>
<li>Understanding IO for WDL, eliminating boilerplate</li>
<li>Matrix testing to increase test depth</li>
<li>Advanced builtin assertions</li>
</ol>
<p>The third point is more aspirational than concrete for the initial release. See <a href="#future-possibilities">future-possibilities</a> for elaboration.</p>
<p>REVIEWERS: I can elaborate further if asked</p>
<h2 id="wdl-ci"><a class="header" href="#wdl-ci"><a href="https://github.com/DNAstack/wdl-ci">wdl-ci</a></a></h2>
<p>I found this tool while looking for existing frameworks when starting this document; which is to say it‚Äôs new to me and I have not tried running it, but it has some interesting capabilities.</p>
<p>This is <em>not</em> a unit testing framework, and looks geared towards system testing or end-to-end testing, or whatever term you want to use for ensuring <em>consistency and reproducibility</em> while developing a workflow. Definitely something worth revisiting when we circle back to that use case, but at the moment this is just designed for a different use than my proposal.</p>
<h2 id="pytest-wdl"><a class="header" href="#pytest-wdl"><a href="https://github.com/EliLillyCo/pytest-wdl">pytest-wdl</a></a></h2>
<p>Last update was 4 years ago, which we on the <code>workflows</code> repo considered a deal breaker when we were initially shopping for CI testing. It seems very similar to pytest-workflow, at least on the surface, (of course they are both plugins to the popular pytest framework) but admittedly I have not dug very deep into this project.</p>
<h2 id="cz-id-repo"><a class="header" href="#cz-id-repo"><a href="https://github.com/chanzuckerberg/czid-workflows/tree/main?tab=readme-ov-file#cicd">CZ ID repo</a></a></h2>
<p>This is just a WDL repo, not a full test framework, but they do have a bespoke CI/CD set up that I reviewed. Uses miniwdl under the hood and seems worth mentioning, but not a generalizable approach.</p>
<h1 id="future-possibilities"><a class="header" href="#future-possibilities">Future possibilities</a></h1>
<h2 id="builtin-tests"><a class="header" href="#builtin-tests">Builtin tests</a></h2>
<p>I think there‚Äôs a lot of room for growth in the builtin test conditions. This document just has what I think are about appropriate for an initial release (i.e. are relatively easy to implement), but that shouldn‚Äôt be the end of the builtin tests. I can imagine us writing bioinformatics specific tests using the <code>noodles</code> library for testing things like ‚Äúis this output file a valid BAM?‚Äù, ‚Äúis this BAM coordinate sorted?‚Äù, ‚Äúis the output BAI file correctly matched to the output BAM?‚Äù, and many many more such tests.</p>
<h2 id="adoption-by-the-wdl-specification"><a class="header" href="#adoption-by-the-wdl-specification">Adoption by the WDL specification?</a></h2>
<p>TOML based test definitions could be lifted over to WDL <code>meta</code> sections if this approach to testing proves valuable. This RFC is concerned with an <em>external</em> testing framework, but this possibility could be explored further down the road.</p>
<h2 id="validating-other-engines"><a class="header" href="#validating-other-engines">Validating other engines</a></h2>
<p>First off, I don‚Äôt think this is something we want to pursue within Sprocket, but I didn‚Äôt want to omit the possibility from this document.</p>
<p>Supporting multiple engines/runners/environments/etc. is valuable and something many WDL authors are looking for. In the <code>workflows</code> repo, we currently validate our tasks with both <code>sprocket run</code> and <code>miniwdl run</code>; ideally we‚Äôd like to expand that to include others as well, but it is tricky to get running smoothly.</p>
<p>To be blunt, I think this is out of scope for what Sprocket should be focusing on. An existing ‚Äúgeneralized‚Äù framework (like pytest-workflow) would be better suited for this kind of validation.</p>
<h2 id="individual-test-files"><a class="header" href="#individual-test-files">Individual test files</a></h2>
<p>An annoyance for me while working on the <code>workflows</code> CI (with pytest-workflow) is that I often have to write individual input JSON files that are then pointed to in the test definition with a relative path. This meant opening two files to figure out what a test was doing; and the pathing was a pain due to our repo structure and the differing path resolution of Sprocket and miniwdl. This proposal aimed to keep all the relevant test information colocated in a single TOML table, but that does create a restriction where the inputs can‚Äôt be trivially used in a different context.</p>
<p>We could explore an alternative syntax that allows test inputs to be defined separately from the test.</p>
<h2 id="integration-of-custom-assertions"><a class="header" href="#integration-of-custom-assertions">Integration of custom assertions</a></h2>
<p>The current proposal for custom assertions is pretty bare bones. This allows for a great deal of flexibility at very little implementation complexity, but we may want to offer tighter integration in the future. Maybe instead of invoking plain executables, we could integrate Python in some way? Calling out Python explicitly, as it is a popular (particularly among bioinformaticians) and flexible language. However environment management with Python dependencies can be a bit of a nightmare, and I‚Äôm not really sure of an ergonomic way we could integrate that.</p>
<h2 id="e2e-testing"><a class="header" href="#e2e-testing">E2E testing</a></h2>
<p>As stated in the ‚Äúmotivation‚Äù section, this proposal is ignoring end-to-end (or E2E) tests and is really just focused on enabling unit testing for CI purposes. Perhaps some of this could be re-used for an E2E API, but I have largely ignored that aspect. (Also I have lots of thoughts about what that might look like, but for brevity will not elaborate further.)</p>
<h2 id="caching"><a class="header" href="#caching">Caching</a></h2>
<p>At the time of writing, Sprocket does not yet have a call caching feature. But once that feature lands, it will prove useful for this framework as a way to reduce runtime on subsequent test runs.</p>
<div style="break-before: page; page-break-before: always;"></div>
<ul>
<li>Feature Name: call-caching</li>
<li>Start Date: 2025-10</li>
</ul>
<h1 id="summary-1"><a class="header" href="#summary-1">Summary</a></h1>
<p>The Sprocket call caching feature enables <code>sprocket run</code> to skip executing
tasks that have been previously executed successfully and instead reuse the
last known outputs of the task.</p>
<h1 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h1>
<p>Sprocket currently cannot resume a failed or canceled workflow, meaning that it
must re-execute every task that completed successfully on a prior run of that
workflow.</p>
<p>As tasks can be very expensive to execute, in terms of both compute resources
and time, this can be a major barrier to using Sprocket in the bioinformatics
space.</p>
<p>The introduction of caching task outputs so that they can be reused in lieu of
re-executing a task will allow Sprocket to quickly resume a workflow run and
reduce redundant execution.</p>
<h1 id="cache-key"><a class="header" href="#cache-key">Cache Key</a></h1>
<p>The cache key will a <a href="https://github.com/BLAKE3-team/BLAKE3">Blake3</a> digest derived from hashing the following:</p>
<ol>
<li>The WDL document URI <a href="#hashing-internal-strings">string</a>.</li>
<li>The task name as a <a href="#hashing-internal-strings">string</a>.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">name</a>,
<a href="#hashing-wdl-values">value</a>) pairs that make up the task‚Äôs inputs, ordered
lexicographically by name.</li>
</ol>
<p>This implies that the task‚Äôs cache key is sensitive to the <em><strong>WDL document being
moved</strong></em>, <em><strong>the task being renamed</strong></em>, or the <em><strong>input values to the task
changing</strong></em>; any of the above will cause a cache miss and Sprocket will not
distinguish the cause for a cache miss when the key changes.</p>
<p>The result is a 32 byte Blake3 digest that can be represented as a
lowercase hexadecimal string, (e.g.
<code>295192ea1ec8566d563b1a7587e5f0198580cdbd043842f5090a4c197c20c67a</code>) for the
purpose of cache entry file names.</p>
<h1 id="call-cache-directory"><a class="header" href="#call-cache-directory">Call Cache Directory</a></h1>
<p>Once a cache key is calculated, it can be used to locate an entry within the
call cache directory.</p>
<p>The call cache directory may be configured via <code>sprocket.toml</code>:</p>
<pre><code class="language-toml">[run.task]
cache_dir = "&lt;path_to_cache&gt;"
</code></pre>
<p>The default call cache location will be the user‚Äôs cache directory joined with
<code>./sprocket/calls</code>.</p>
<p>The call cache directory will contain an empty <code>.lock</code> file that will be used
to acquire shared and exclusive file locks on the <em>entire</em> call cache; the lock
file serves to coordinate access between <code>sprocket run</code> and a future <code>sprocket clean</code>
command.</p>
<p>During the execution of <code>sprocket run</code>, only a single <em>shared lock</em> will be
acquired on the <code>.lock</code> file and kept for the entirety of the run.</p>
<p>The call cache will have no eviction policy, meaning it will grow unbounded. A
future <code>sprocket clean</code> command might give statistics of current cache sizes
with the option to clean them, if desired. The <code>sprocket clean</code> command would
take an <em>exclusive lock</em> on the <code>.lock</code> file to block any <code>sprocket run</code>
command from executing while it is operating.</p>
<p>Each entry within the call cache will be a file with the same name of the
task‚Äôs cache key.</p>
<p>During a lookup of an entry in the cache, a <em>shared lock</em> will be acquired on
the individual entry file. During the updating of an entry in the cache, an
<em>exclusive lock</em> will be acquired on the individual entry file.</p>
<p>The entry file will contain a JSON object with the following information:</p>
<pre><code class="language-javascript">{
  "version": 1,                          // A monotonic version for the entry format.
  "command": "&lt;string-digest&gt;",          // The digest of the task's evaluated command.
  "container": "&lt;string&gt;",               // The container used by the task.
  "shell": "&lt;string&gt;",                   // The shell used by the task.
  "requirements": {
    "&lt;key&gt;": "&lt;value-digest&gt;",           // The requirement key and value digest
    // ...
  },
  "hints": {
    "&lt;key&gt;": "&lt;value-digest&gt;",           // The hint key and value digest
    // ...
  },
  "inputs": {
    "&lt;path-or-url&gt;": "&lt;content-digest&gt;", // The previous backend input and its content digest
    // ...
  },
  "exit": 0,                             // The last exit code of the task.
  "stdout": {
    "location": "&lt;path-or-url&gt;",         // The location of the last stdout output.
    "digest": "&lt;content-digest&gt;".        // The content digest of the last stdout output.
  },
  "stderr": {
    "location": "&lt;path-or-url&gt;",         // The location of the last stderr output.
    "digest": "&lt;content-digest&gt;"         // The content digest of the last stderr output.
  },
  "work": {
    "location": "&lt;path-or-url&gt;",         // The location of the last working directory.
    "digest": "&lt;content-digest&gt;"         // The content digest of the last working directory.
  }
}
</code></pre>
<p><em>Note: as a cache entry may contain absolute paths pointing at files in the
<code>runs</code> directory, deleting or moving a <code>runs</code> directory may invalidate entries
in the call cache.</em></p>
<p>See the <a href="#cache-entry-digests">section on cache entry digests</a> for information
on how the digests in the cache entry file are calculated.</p>
<h1 id="call-cache-hit"><a class="header" href="#call-cache-hit">Call Cache Hit</a></h1>
<p>Checking for a cache hit acquires a <em>shared lock</em> on the call cache entry file.</p>
<p>A cache entry lookup only occurs for the <em>first</em> execution of the task; the
call cache is skipped for subsequent retries of the task.</p>
<p>A call cache hit will occur if all of the following criteria are met:</p>
<ul>
<li>A file with the same name as the task‚Äôs cache key is present in the call cache
directory and the file can be deserialized to the expected JSON object.</li>
<li>The cache entry‚Äôs <code>version</code> field matches the cache version expected by
Sprocket.</li>
<li>The digest of the currently executing task‚Äôs evaluated command matches the
cache entry‚Äôs <code>command</code> field.</li>
<li>The container used by the task matches the cache entry‚Äôs <code>container</code> field.</li>
<li>The shell used by the task matches the cache entry‚Äôs <code>shell</code> field.</li>
<li>The digests of the task‚Äôs requirements exactly match those in the cache
entry‚Äôs <code>requirements</code> field.</li>
<li>The digests of the task‚Äôs hints exactly match those in the cache entry‚Äôs
<code>hints</code> field.</li>
<li>The digests of the task‚Äôs backend inputs exactly match those in the cache
entry‚Äôs <code>inputs</code> field.</li>
<li>The digest of the cache entry‚Äôs <code>stdout</code> field matches the current digest of
its location.</li>
<li>The digest of the cache entry‚Äôs <code>stdout</code> field matches the current digest of
its location.</li>
<li>The digest of the cache entry‚Äôs <code>work</code> field matches the current digest of
its location.</li>
</ul>
<p>If any of the criteria above are not met, the failing criteria is logged (e.g.
‚Äúentry not present in the cache‚Äù, ‚Äúcommand was modified‚Äù, ‚Äúinput was modified‚Äù,
‚Äústdout file was modified‚Äù, etc.) and it is treated as a cache miss.</p>
<p>Upon a call cache hit, a <code>TaskExecutionResult</code> will be created from the
<code>stdout</code>, <code>stderr</code>, and <code>work</code> fields of the cache entry and task execution
will be skipped.</p>
<h1 id="call-cache-miss"><a class="header" href="#call-cache-miss">Call Cache Miss</a></h1>
<p>Upon a call cache miss, the task will be executed by passing the request to the
task execution backend.</p>
<p>After the task successfully executes <em>on its first attempt only</em>, the following
occurs:</p>
<ul>
<li>Content digests will be calculated for <code>stdout</code>, <code>stderr</code>, and <code>work</code> of the
execution result returned by the execution backend.</li>
<li>An <em>exclusive lock</em> is acquired on the cache entry file.</li>
<li>The new cache entry is JSON serialized into the cache entry file.</li>
</ul>
<p>If a task fails to execute on its first attempt, the task‚Äôs cache entry will
not be updated regardless of a successful retry.</p>
<p><em><strong>Note: a non-zero exit code of a task‚Äôs execution is not inherently a failure
as the WDL task may specify permissible non-zero exit codes.</strong></em></p>
<h1 id="cache-entry-digests"><a class="header" href="#cache-entry-digests">Cache Entry Digests</a></h1>
<p>A cache entry may contain three different types of digests as lowercase
hexadecimal strings:</p>
<ul>
<li>A digest produced by <a href="#hashing-internal-strings">hashing an internal string</a>.</li>
<li>A digest produced by <a href="#hashing-wdl-values">hashing a WDL value</a>.</li>
<li>A <a href="#content-digests">content digest</a> of a backend input.</li>
</ul>
<p><a href="https://github.com/BLAKE3-team/BLAKE3">Blake3</a> will be used as the hash algorithm for producing the digests.</p>
<h2 id="hashing-internal-strings"><a class="header" href="#hashing-internal-strings">Hashing Internal Strings</a></h2>
<p>Hashing an internal string (i.e. a string used internally by the engine) will
update the hasher with:</p>
<ol>
<li>A four byte length value in little endian order.</li>
<li>The UTF-8 bytes representing the string.</li>
</ol>
<h2 id="hashing-wdl-values"><a class="header" href="#hashing-wdl-values">Hashing WDL Values</a></h2>
<p>For hashing the values of the <code>requirements</code> and <code>hints</code> section, a <a href="https://github.com/BLAKE3-team/BLAKE3">Blake3</a>
hasher will be updated as described in this section.</p>
<p>Compound values will recursively hash their contained values.</p>
<h3 id="hashing-a-none-value"><a class="header" href="#hashing-a-none-value">Hashing a <code>None</code> value</a></h3>
<p>A <code>None</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>0</code> to indicate a <code>None</code> variant.</li>
</ol>
<h3 id="hashing-a-boolean-value"><a class="header" href="#hashing-a-boolean-value">Hashing a <code>Boolean</code> value</a></h3>
<p>A <code>Boolean</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>1</code> to indicate a <code>Boolean</code> variant.</li>
<li>A byte with a value of <code>1</code> if the value is <code>true</code> or <code>0</code> if the value is
<code>false</code>.</li>
</ol>
<h3 id="hashing-an-int-value"><a class="header" href="#hashing-an-int-value">Hashing an <code>Int</code> value</a></h3>
<p>An <code>Int</code> value will update the hasher with:</p>
<ol>
<li>A byte with a a value of <code>2</code> to indicate an <code>Int</code> variant.</li>
<li>An 8 byte value representing the signed integer in little endian order.</li>
</ol>
<h3 id="hashing-a-float-value"><a class="header" href="#hashing-a-float-value">Hashing a <code>Float</code> value</a></h3>
<p>A <code>Float</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>3</code> to indicate a <code>Float</code> variant.</li>
<li>An 8 byte value representing the float in little endian order.</li>
</ol>
<h3 id="hashing-a-string-value"><a class="header" href="#hashing-a-string-value">Hashing a <code>String</code> value</a></h3>
<p>A <code>String</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>4</code> to indicate a <code>String</code> variant.</li>
<li>The <a href="#hashing-internal-strings">internal string</a> value of the <code>String</code>.</li>
</ol>
<h3 id="hashing-a-file-value"><a class="header" href="#hashing-a-file-value">Hashing a <code>File</code> value</a></h3>
<p>A <code>File</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>5</code> to indicate a <code>File</code> variant.</li>
<li>The <a href="#hashing-internal-strings">internal string</a> value of the <code>File</code>.</li>
</ol>
<p>For the purpose of hashing a <code>File</code> value, the contents of the file specified
by the value are <em>not</em> considered.</p>
<p>If the <code>File</code> is a backend input, the contents will be taken into consideration
when backend input content digests are produced.</p>
<h3 id="hashing-a-directory-value"><a class="header" href="#hashing-a-directory-value">Hashing a <code>Directory</code> value</a></h3>
<p>A <code>Directory</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>6</code> to indicate a <code>Directory</code> variant.</li>
<li>The <a href="#hashing-internal-strings">internal string</a> value of the <code>Directory</code>.</li>
</ol>
<p>For the purpose of hashing a <code>Directory</code> value, the contents of the directory
specified by the value are <em>not</em> considered.</p>
<p>If the <code>Directory</code> is a backend input, the contents will be taken into
consideration when backend input content digests are produced.</p>
<h3 id="hashing-a-pair-value"><a class="header" href="#hashing-a-pair-value">Hashing a <code>Pair</code> value</a></h3>
<p>A <code>Pair</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>7</code> to indicate a <code>Pair</code> variant.</li>
<li>The recursive hash of the <code>left</code> <a href="#hashing-wdl-values">value</a>.</li>
<li>The recursive hash of the <code>right</code> <a href="#hashing-wdl-values">value</a>.</li>
</ol>
<h3 id="hashing-an-array-value"><a class="header" href="#hashing-an-array-value">Hashing an <code>Array</code> value</a></h3>
<p>An <code>Array</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>8</code> to indicate an <code>Array</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of <a href="#hashing-wdl-values">elements</a>
contained in the array, in insertion order.</li>
</ol>
<h4 id="hashing-a-map-value"><a class="header" href="#hashing-a-map-value">Hashing a <code>Map</code> value</a></h4>
<p>A <code>Map</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>9</code> to indicate a <code>Map</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-wdl-values">key</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in insertion order.</li>
</ol>
<h4 id="hashing-an-object-value"><a class="header" href="#hashing-an-object-value">Hashing an <code>Object</code> value</a></h4>
<p>An <code>Object</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>10</code> to indicate an <code>Object</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">key</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in insertion order.</li>
</ol>
<h3 id="hashing-a-struct-value"><a class="header" href="#hashing-a-struct-value">Hashing a <code>Struct</code> value</a></h3>
<p>A <code>Struct</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>11</code> to indicate a <code>Struct</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">field name</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in field declaration order.</li>
</ol>
<h3 id="hashing-a-hints-value-wdl-12"><a class="header" href="#hashing-a-hints-value-wdl-12">Hashing a <code>hints</code> value (WDL 1.2+)</a></h3>
<p>A <code>hints</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>12</code> to indicate a <code>hints</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">key</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in insertion order.</li>
</ol>
<h3 id="hashing-an-input-value-wdl-12"><a class="header" href="#hashing-an-input-value-wdl-12">Hashing an <code>input</code> value (WDL 1.2+)</a></h3>
<p>An <code>input</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>13</code> to indicate an <code>input</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">key</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in insertion order.</li>
</ol>
<h3 id="hashing-an-output-value-wdl-12"><a class="header" href="#hashing-an-output-value-wdl-12">Hashing an <code>output</code> value (WDL 1.2+)</a></h3>
<p>An <code>output</code> value will update the hasher with:</p>
<ol>
<li>A byte with a value of <code>14</code> to indicate an <code>output</code> variant.</li>
<li>The <a href="#hashing-sequences">sequence</a> of (<a href="#hashing-internal-strings">key</a>, <a href="#hashing-wdl-values">value</a>)
pairs, in insertion order.</li>
</ol>
<h3 id="hashing-sequences"><a class="header" href="#hashing-sequences">Hashing Sequences</a></h3>
<p>Hashing a <em>sequence</em> will update the hasher with:</p>
<ol>
<li>A four byte length value in little endian order.</li>
<li>The hash of each element in the sequence.</li>
</ol>
<h2 id="content-digests"><a class="header" href="#content-digests">Content Digests</a></h2>
<p>As <code>wdl-engine</code> already calculates digests of files and directories for
uploading files to cloud storage, the call caching implementation will make use
of the existing content digest cache, with some improvements.</p>
<p>Keep in mind that <code>File</code> and <code>Directory</code> values may be either local file paths
(e.g. <code>/foo/bar.txt</code>) or remote URLs (e.g. <code>https://example.com/bar.txt</code>,
<code>s3://foo/bar.txt</code>, etc.).</p>
<h3 id="local-file-digests"><a class="header" href="#local-file-digests">Local File Digests</a></h3>
<p>Calculating the content digest of a local file is as simple as feeding every
byte of the file‚Äôs contents to a <a href="https://github.com/BLAKE3-team/BLAKE3">Blake3</a> hasher; functions that <code>mmap</code>
large files to calculate the digest will also be utilized.</p>
<h3 id="remote-file-digests"><a class="header" href="#remote-file-digests">Remote File Digests</a></h3>
<p>A <code>HEAD</code> request will be made for the remote file URL.</p>
<p>If the remote URL is for a supported cloud storage service, the response is
checked for the appropriate metadata header (e.g. <code>x-ms-meta-content_digest</code>,
<code>x-amz-meta-content-digest</code>, or <code>x-goog-meta-content-digest</code>) and the header is
treated like a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Content-Digest"><code>Content-Digest</code></a> header.</p>
<p>Otherwise, the response must have either a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Content-Digest"><code>Content-Digest</code></a>
header or a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/ETag">strong <code>ETag</code></a> header. If the response does not have the
required header or if the header‚Äôs value is invalid, it is treated as a failure
to calculate the content digest.</p>
<p>If the <code>HEAD</code> request is unsuccessful and the error is considered to be a
‚Äútransient‚Äù failure (e.g. a 500 response), the <code>HEAD</code> request is retried
internally up to some configurable limit. If the request is unsuccessful after
exhausting the retries, it is treated as a failure to calculate the content
digest.</p>
<p>If a <code>Content-Digest</code> header was returned, the hasher is updated with:</p>
<ul>
<li>A <code>0</code> byte to indicate the header was <code>Content-Digest</code>.</li>
<li>The <a href="#hashing-internal-strings">algorithm string</a> of the header.</li>
<li>The <a href="#hashing-sequences">sequence of digest bytes</a> of the header.</li>
</ul>
<p>If an <code>ETag</code> header was returned, the hasher is updated with:</p>
<ul>
<li>A <code>1</code> byte to indicate the header was <code>ETag</code>.</li>
<li>The <a href="#hashing-internal-strings">strong ETag header value string</a>.</li>
</ul>
<p>Note that Sprocket will <em>not</em> verify that the content digest reported by the
header matches the actual content digest of the file as that requires
downloading the file‚Äôs entire contents.</p>
<h3 id="local-directory-digests"><a class="header" href="#local-directory-digests">Local Directory Digests</a></h3>
<p>The content digest of a local directory is calculated by recursively walking
the directory in a consistent order and updating a Blake3 hasher based on each
entry of the directory.</p>
<p>A directory‚Äôs entry is hashed with:</p>
<ol>
<li>The <a href="#hashing-internal-strings">relative path</a> of the directory entry.</li>
<li>A <code>0</code> byte if the entry is a file or <code>1</code> if it is a directory.</li>
<li>If the entry is a file, the hasher is updated with the contents of the file.</li>
</ol>
<p>Finally, a four byte (little endian) entry count value is written to the hasher
before it is finalized to produce the 32 byte Blake3 content digest of the
directory.</p>
<p><em><strong>Note: it is an error if the directory contains a symbolic link to a
directory that creates a cycle (i.e. to an ancestor of the directory being
hashed).</strong></em></p>
<h3 id="remote-directory-digests"><a class="header" href="#remote-directory-digests">Remote Directory Digests</a></h3>
<p><code>cloud-copy</code> has the facility to walk a ‚Äúdirectory‚Äù cloud storage URL; it uses
the specific cloud storage API to list all objects that start with the
directory‚Äôs prefix.</p>
<p>The content digest of a remote directory is calculated by using <code>cloud-copy</code> to
walk the directory in a consistent order and then updating a Blake3 hasher
based on each entry of the directory.</p>
<p>A directory‚Äôs entry is hashed with:</p>
<ol>
<li>The <a href="#hashing-internal-strings">relative path</a> of the entry from the base
URL.</li>
<li>The 32 byte Blake3 digest of the <a href="#remote-file-digests">remote file entry</a>.</li>
</ol>
<p>Finally, a four byte (little endian) entry count value is written to the hasher
before it is finalized to produce the 32 byte Blake3 content digest of the
directory.</p>
<h1 id="enabling-call-caching"><a class="header" href="#enabling-call-caching">Enabling Call Caching</a></h1>
<p>A setting in <code>sprocket.toml</code> can control whether or not call caching is
enabled for every invocation of <code>sprocket run</code>:</p>
<pre><code class="language-toml">[run.task]
cache = "off|on|explicit" # defaults to `off`
</code></pre>
<p>The supported values for the <code>cache</code> setting are:</p>
<ul>
<li><code>off</code> - do not check the call cache or write new cache entries at all.</li>
<li><code>on</code> - check the call cache and write new cache entries for all tasks except
those that have a <code>cacheable: false</code> hint.</li>
<li><code>explicit</code> - check the call cache and write new cache entries <em>only</em> for
tasks that have a <code>cacheable: true</code> hint.</li>
</ul>
<p>Sprocket will default the setting to <code>off</code> as it safer to let users consciously
opt-in than potentially serve stale results from the cache without the user‚Äôs
knowledge that call caching is occurring.</p>
<h2 id="opting-out"><a class="header" href="#opting-out">Opting Out</a></h2>
<p>When call caching has been enabled, users may desire to opt-out of call caching
for individual tasks or a single <code>sprocket run</code> invocation.</p>
<h3 id="task-opt-out"><a class="header" href="#task-opt-out">Task Opt Out</a></h3>
<p>An individual task may opt out of call caching through the use of the
<code>cacheable</code> hint:</p>
<pre><code class="language-wdl">hints {
  "cacheable": false
}
</code></pre>
<p>The <code>cacheable</code> hint defaults to <code>false</code> if the <code>task.cache</code> setting is
<code>explicit</code>; otherwise, the hint defaults to <code>true</code>.</p>
<p>When <code>cacheable</code> is <code>false</code>, the call cache is not checked prior to task
execution and the result of the task‚Äôs execution is not cached.</p>
<h3 id="run-opt-out"><a class="header" href="#run-opt-out">Run Opt Out</a></h3>
<p>A single invocation of <code>sprocket run</code> may pass the <code>--no-call-cache</code> option.</p>
<p>Doing so disables the use of the call cache for that specific run, both in
terms of looking up results and storing results in the cache.</p>
<h1 id="failure-modes-for-sprocket"><a class="header" href="#failure-modes-for-sprocket">Failure Modes for Sprocket</a></h1>
<p>Sprocket currently uses a <em>fail fast</em> failure mode where Sprocket immediately
attempts to cancel any currently executing tasks and return the error it
encountered. This is also the behavior of the user invoking <code>Ctrl-C</code> to
interrupt evaluation.</p>
<p>Failing this way may cancel long-running tasks that would otherwise have
succeeded and subsequently prevent caching results for those tasks.</p>
<p>To better support call caching, Sprocket should be enhanced to support a <em>fail
slow</em> failure mode (the new default), with users able to configure Sprocket to
use the previous <em>fail fast</em> behavior when desired.</p>
<p>With a <em>fail slow</em> failure mode, currently executing tasks are awaited to
completion, and their successful results are cached before attempting to abort
the run.</p>
<p>This also changes how Sprocket handles <code>Ctrl-C</code>. Sprocket should now support
multiple <code>Ctrl-C</code> invocations depending on its configured failure mode:</p>
<ol>
<li>If the configured failure mode is <em>fail slow</em>, the user invokes <code>Ctrl-C</code> and
Sprocket prints a message informing the user that it is waiting on
outstanding task executions to complete and to hit <code>Ctrl-C</code> again to cancel
tasks instead. It then proceeds to wait for executing tasks to complete to
cache successful results.</li>
<li>The user invokes <code>Ctrl-C</code> and Sprocket prints a message informing the user
that it is now canceling the executing tasks and to hit <code>Ctrl-C</code> again to
immediately terminate Sprocket. It then proceeds to cancel the executing
tasks and wait for the cancellations to occur.</li>
<li>The user invokes <code>Ctrl-C</code> and Sprocket immediately errors with a ‚Äúevaluation
aborted‚Äù error message.</li>
</ol>
<p>The failure mode can be configured via <code>sprocket.toml</code>:</p>
<pre><code class="language-toml">[run]
fail = "slow|fast"   # Defaults to `slow`
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="provenance-tracking-and-analysis-management"><a class="header" href="#provenance-tracking-and-analysis-management">Provenance Tracking and Analysis Management</a></h1>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#summary-2">Summary</a></li>
<li><a href="#motivation-2">Motivation</a></li>
<li><a href="#architecture-overview">Architecture Overview</a></li>
<li><a href="#database-schema">Database Schema</a>
<ul>
<li><a href="#sqlite-configuration">SQLite Configuration</a></li>
<li><a href="#metadata-table">Metadata Table</a></li>
<li><a href="#invocations-table">Invocations Table</a></li>
<li><a href="#workflows-table">Workflows Table</a></li>
<li><a href="#index-log-table">Index Log Table</a></li>
<li><a href="#concurrency">Concurrency</a></li>
</ul>
</li>
<li><a href="#directory-structure">Directory Structure</a>
<ul>
<li><a href="#output-directory-layout">Output Directory Layout</a></li>
<li><a href="#directory-behaviors">Directory Behaviors</a></li>
</ul>
</li>
<li><a href="#index-functionality">Index Functionality</a>
<ul>
<li><a href="#index-creation">Index Creation</a></li>
<li><a href="#symlinking-behavior">Symlinking Behavior</a></li>
</ul>
</li>
<li><a href="#cli-workflow">CLI Workflow</a>
<ul>
<li><a href="#execution-flow">Execution Flow</a></li>
<li><a href="#cli-flags">CLI Flags</a></li>
<li><a href="#key-behaviors">Key Behaviors</a></li>
</ul>
</li>
<li><a href="#server-mode">Server Mode</a>
<ul>
<li><a href="#server-configuration">Server Configuration</a></li>
<li><a href="#execution-flow-1">Execution Flow</a></li>
<li><a href="#rest-api-endpoints">REST API Endpoints</a></li>
</ul>
</li>
<li><a href="#rationale-and-alternatives-1">Rationale and Alternatives</a>
<ul>
<li><a href="#why-sqlite">Why SQLite?</a></li>
<li><a href="#future-work-postgresql-support">Future Work: PostgreSQL Support</a></li>
<li><a href="#alternative-embedded-key-value-stores-rocksdb-lmdb">Alternative: Embedded Key-Value Stores (RocksDB, LMDB)</a></li>
<li><a href="#alternative-filesystem-only-no-database">Alternative: Filesystem Only (No Database)</a></li>
</ul>
</li>
</ul>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>This RFC proposes a comprehensive provenance tracking and analysis management system for Sprocket built on the principle of progressive disclosure. The system automatically tracks all workflow executions in a SQLite database while maintaining a dual filesystem organization: a complete provenance record preserving every execution detail in chronological directories (<code>runs/</code>), and an optional user-defined logical index (<code>index/</code>) that symlinks outputs into domain-specific hierarchies (e.g., by project or analysis type). This approach scales from simple single-workflow use cases to production analysis management systems handling thousands of samples, providing both the auditability of complete execution history and the usability of custom organization without requiring users to choose one or maintain both manually.</p>
<h2 id="motivation-2"><a class="header" href="#motivation-2">Motivation</a></h2>
<p>Sprocket currently focuses on executing workflows and producing outputs, but several critical aspects of production bioinformatics work remain unaddressed. The current output directory structure, organized solely by workflow name and timestamp, makes it difficult for users to find their results, particularly when managing multiple projects. While a complete provenance filesystem organized by execution time provides valuable auditability‚Äîpreserving every execution detail, retry attempt, and task output in a structured hierarchy‚Äîthis very completeness creates organizational complexity. Outputs are scattered across timestamped directories deep within nested task execution paths, with no logical organization by project or data type. Users need both the complete provenance record for reproducibility and a simplified, domain-specific view for everyday access. Currently, they must choose one or the other, or maintain both manually through external scripts. There is also no way to track all workflows run against a given sample or understand analysis lineage over time.</p>
<p>Additionally, Sprocket maintains no persistent record of execution history. Users cannot query which workflows were run, when they executed, what inputs were provided, or who submitted them. This lack of provenance tracking makes it impossible to audit past analyses or understand the evolution of results. Furthermore, there is no real-time visibility into running workflows across multiple submissions, making it difficult to monitor the overall state of an analysis pipeline or identify bottlenecks.</p>
<p>Existing solutions in the broader ecosystem fall into two categories. Lightweight engines handle execution well but leave tracking and organization to external tools that users must discover, install, and configure separately. Enterprise analysis management systems provide comprehensive tracking but require significant infrastructure to deploy, creating a barrier to entry that prevents users from simply trying them out.</p>
<p>This RFC proposes a middle path through <a href="https://en.wikipedia.org/wiki/Progressive_disclosure"><strong>progressive disclosure</strong></a>: Sprocket provides sophisticated analysis management capabilities that activate automatically as users need them, without requiring upfront infrastructure or configuration.</p>
<h3 id="user-journey"><a class="header" href="#user-journey">User Journey</a></h3>
<p>A user learns about Sprocket and wants to try a yak shaving workflow. They have a yak named Fluffy who desperately needs a haircut, so they download Sprocket and run:</p>
<pre><code class="language-bash">sprocket run yak_shaving.wdl -i defaults.json yak_name=fluffy style=mohawk
</code></pre>
<p>A directory called <code>out/</code> is created automatically in their current working directory. Within it, they find their workflow outputs organized in <code>out/runs/yak_shaving/&lt;timestamp&gt;/</code>, where the timestamp corresponds to when they ran the workflow. A database at <code>out/database.db</code> silently tracks the execution, but the user doesn‚Äôt need to think about it. They have their results as returned in the output JSON. This is the Sprocket ‚Äúlight-disclosure‚Äù experience‚Äîworking results with zero configuration.</p>
<p>The user finds this approach easy and begins to wonder if Sprocket can style all yaks in their herd. Beyond just performing the styling, the user hopes Sprocket can organize the yak satisfaction surveys for long-term safe keeping.</p>
<p>Looking through the documentation, they discover the <code>--index-on</code> flag and adapt their workflow:</p>
<pre><code class="language-bash">for YAK in yaks/*; do
  sprocket run yak_shaving.wdl -i defaults.json \
    yak_name=$YAK style=mohawk \
    --index-on "YakProject/2025/$YAK"
done
</code></pre>
<p>As these workflows complete, a new directory structure appears under <code>out/index/YakProject/2025/</code>, with each yak‚Äôs photos and satisfaction surveys organized in subdirectories by yak name. The index contains symlinks pointing back to files in <code>out/runs/</code>, so the historical execution record remains intact while the index provides the logical organization the user cares about. If they rerun a yak‚Äôs styling, the index automatically updates to point to the new results while the database‚Äôs <code>index_log</code> table preserves the complete history of what was indexed at each point in time. The entire directory structure is portable‚Äîmoving the <code>out/</code> directory with <code>mv</code> preserves all relationships. With just one additional flag, they‚Äôve unlocked organized output management across their entire herd. This is the Sprocket ‚Äúmedium-disclosure‚Äù experience.</p>
<p>Now satisfied with their organized outputs, the user realizes they‚Äôd like real-time monitoring of running workflows and the ability to submit styling orders remotely from their laptop while execution happens on a shared server. They run:</p>
<pre><code class="language-bash">sprocket server --port 8080
</code></pre>
<p>The HTTP server starts immediately, connecting to the existing database and making all historical runs queryable through a REST API. They can now submit workflows via HTTP and monitor progress through API queries. All workflows submitted through the API are tracked in the same database alongside CLI submissions, and all outputs appear in the same <code>out/</code> directory structure. There‚Äôs no database setup, no configuration files to manage, no migration of historical data‚Äîthey simply started the server and gained remote access and monitoring capabilities.</p>
<p>As the yak grooming business grows, the user‚Äôs team begins submitting hundreds of workflows per day from multiple servers. SQLite‚Äôs single-writer model handles this well initially, but they eventually want to scale to thousands of concurrent submissions with multiple Sprocket servers sharing a central database. At this point, they provision a PostgreSQL database and update their configuration:</p>
<pre><code class="language-toml">[database]
url = "postgresql://postgres:postgres@db.example.com:5432/sprocket"
</code></pre>
<p>Then they run a single command to transfer their existing SQLite data:</p>
<pre><code class="language-bash">sprocket database transfer --from ./out/database.db --to postgresql://postgres:postgres@db.example.com:5432/sprocket
</code></pre>
<p>This transfers all historical workflow executions and index logs to PostgreSQL. The familiar output directory structure remains unchanged‚Äî<code>runs/</code> and <code>index/</code> still live on the filesystem‚Äîbut the database now runs on dedicated infrastructure with MVCC-based concurrency control, enabling unlimited concurrent writers across multiple Sprocket server instances. The user has progressed from zero-configuration local execution to enterprise-scale workflow orchestration, with each transition requiring configuration only when their needs demanded it. This is the Sprocket ‚Äúheavy-disclosure‚Äù experience.</p>
<p>This progression happens naturally as the user‚Äôs needs grow. Each step builds on the previous one, with infrastructure complexity introduced only when scale demands it.</p>
<h2 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h2>
<p>The system consists of three independent but coordinated components sharing a common output directory and database.</p>
<ul>
<li>
<p><strong>CLI Execution Mode</strong> (<code>sprocket run</code>). Direct workflow execution that initializes the output directory on first use, creates the database schema via migrations, executes workflows in-process, writes execution metadata transactionally to the database, and creates index symlinks when specified via <code>--index-on</code>.</p>
</li>
<li>
<p><strong>Server Mode</strong> (<code>sprocket server</code>). An HTTP API server that initializes the output directory on first use if needed, accepts workflow submissions via REST API, executes workflows using the same engine as CLI, provides query endpoints for run history and status, and shares the database with CLI via SQLite WAL mode for concurrency.</p>
</li>
<li>
<p><strong>Output directory</strong> (<code>./out/</code>). Filesystem-based storage containing <code>database.db</code> (a SQLite database tracking all executions), <code>runs/&lt;workflow&gt;/&lt;timestamp&gt;/</code> (execution directories organized by workflow and timestamp), and <code>index/</code> (optional symlinked organization created via <code>--index-on</code>).</p>
</li>
</ul>
<p>All three components use identical database schema and filesystem conventions, enabling seamless interoperability. A workflow submitted via CLI is immediately visible to the server, and vice versa.</p>
<p>To ensure consistent workflow execution behavior between CLI and server modes, we‚Äôll refactor the current evaluation and execution code to use an actor-based architecture. A workflow manager actor will handle workflow lifecycle management, database updates, and index creation through message passing. The actor will spawn one or more Tokio tasks to execute workflows concurrently. For <code>sprocket run</code>, the actor will manage a single workflow execution task. For <code>sprocket server</code>, the same actor implementation will manage multiple concurrent workflow execution tasks, one per submitted workflow. This shared architecture ensures that workflow execution semantics, error handling, and database interactions remain identical regardless of submission method.</p>
<h2 id="database-schema"><a class="header" href="#database-schema">Database Schema</a></h2>
<p>The provenance database will use a simple schema optimized for common queries while keeping implementation straightforward. Each output directory will be versioned to ensure compatibility between different Sprocket releases.</p>
<h3 id="sqlite-configuration"><a class="header" href="#sqlite-configuration">SQLite Configuration</a></h3>
<p>Sprocket will configure SQLite with specific pragma settings to optimize for concurrent access, performance, and data integrity. These settings are divided into two categories:</p>
<p><strong>Persistent settings</strong> (applied once when database is created):</p>
<pre><code class="language-sql">pragma journal_mode = wal;
pragma synchronous = normal;
pragma temp_store = memory;
</code></pre>
<ul>
<li><code>journal_mode = wal</code> enables <a href="https://www.sqlite.org/wal.html">Write-Ahead Logging</a>, allowing multiple concurrent readers with a single writer. This setting persists across all connections and provides the foundation for CLI and server to share the database.</li>
<li><code>synchronous = normal</code> balances durability and performance in WAL mode. The database remains safe from corruption but may lose the most recent transaction in the event of a power failure or system crash.</li>
<li><code>temp_store = memory</code> stores temporary tables in memory for better performance.</li>
</ul>
<p><strong>Per-connection settings</strong> (applied when opening each connection):</p>
<pre><code class="language-sql">pragma foreign_keys = on;
pragma busy_timeout = 5000;
pragma cache_size = 2000;
</code></pre>
<ul>
<li><code>foreign_keys = on</code> enables foreign key constraint enforcement for referential integrity.</li>
<li><code>busy_timeout = 5000</code> configures a 5-second timeout when the database is locked. If a write cannot proceed immediately due to another concurrent write, SQLite will retry for up to 5 seconds before returning an error. This prevents spurious failures during normal concurrent access patterns.</li>
<li><code>cache_size = 2000</code> allocates approximately 8MB for SQLite‚Äôs page cache (assuming 4KB pages), improving query performance.</li>
</ul>
<h3 id="metadata-table"><a class="header" href="#metadata-table">Metadata Table</a></h3>
<p>The <code>metadata</code> table tracks the output directory schema version:</p>
<pre><code class="language-sql">create table if not exists metadata (
  -- Metadata key
  key text primary key,
  -- Metadata value
  value text not null
);

-- Insert schema version
insert into metadata (key, value) values ('schema_version', '1');
</code></pre>
<p>Sprocket checks the <code>schema_version</code> on startup and automatically migrates the database schema to the current version if needed. Migrations are applied atomically to ensure database consistency.</p>
<h3 id="invocations-table"><a class="header" href="#invocations-table">Invocations Table</a></h3>
<p>The <code>invocations</code> table groups related workflow submissions.</p>
<pre><code class="language-sql">create table if not exists invocations (
  -- Unique invocation identifier (UUID v4)
  id text primary key,
  -- How workflows are submitted ‚Äî `cli` or `http`
  submission_method text not null,
  -- User or client that created the invocation
  created_by text,
  -- When the invocation was created
  created_at timestamp not null
);
</code></pre>
<p>Each <code>sprocket run</code> command creates its own invocation with <code>created_by</code> populated from the <code>$USER</code> environment variable or system username. A running <code>sprocket server</code> instance creates a single invocation at startup that is shared by all workflows submitted to that server.</p>
<h3 id="workflows-table"><a class="header" href="#workflows-table">Workflows Table</a></h3>
<p>The <code>workflows</code> table tracks individual workflow executions.</p>
<pre><code class="language-sql">create table if not exists workflows (
  -- Unique run identifier (UUID v4)
  id text primary key,
  -- A link to the invocation that created this workflow
  invocation_id text not null,
  -- Workflow name extracted from WDL document
  name text not null,
  -- Workflow source (file path, URL, or git reference)
  source text not null,
  -- Current execution status ‚Äî `pending`, `running`, `completed`, or `failed`
  status text not null,
  -- JSON-serialized workflow inputs
  inputs text,
  -- JSON-serialized workflow outputs (`null` until completion)
  outputs text,
  -- Error message if status is `failed` (`null` otherwise)
  error text,
  -- Relative path to execution directory from `database.db` (e.g., `runs/workflow_name/2025-11-07_143022123456`)
  execution_dir text not null,
  -- When run record was created
  created_at timestamp not null,
  -- When execution started (`null` if still pending)
  started_at timestamp,
  -- When execution finished, success or failure (`null` if still running)
  completed_at timestamp,
  foreign key (invocation_id) references invocations(id)
);
</code></pre>
<h3 id="index-log-table"><a class="header" href="#index-log-table">Index Log Table</a></h3>
<p>The <code>index_log</code> table tracks the history of index symlink updates.</p>
<pre><code class="language-sql">create table if not exists index_log (
  -- Unique log entry identifier (UUID v4)
  id text primary key,
  -- Path within the index directory (e.g., `YakProject/2025/Fluffy/final_photo.jpg`)
  index_path text not null,
  -- Target path relative to `database.db` that the symlink points to (e.g., `runs/yak_shaving/2025-11-07_143022123456/calls/trim_and_style/attempts/0/work/final_photo.jpg`)
  target_path text not null,
  -- Foreign key to `workflows.id` (which workflow created this symlink)
  workflow_id text not null,
  -- When this symlink was created or updated
  created_at timestamp not null,
  foreign key (workflow_id) references workflows(id)
);
</code></pre>
<p>Each time a workflow creates or updates a symlink in the index (via <code>--index-on</code>), a record is inserted into this table. For workflows that update an existing index path, both the old and new symlink targets are preserved in the log, enabling complete historical tracking. Users can query this table to determine what data was indexed at any point in time by finding the most recent log entry before a given date.</p>
<h3 id="concurrency"><a class="header" href="#concurrency">Concurrency</a></h3>
<p>SQLite operates in <a href="https://www.sqlite.org/wal.html">WAL (Write-Ahead Logging) mode</a>, which allows multiple concurrent readers, one writer at a time (writes are serialized), and readers to access the database during writes. This enables CLI and server to operate simultaneously on the same database without coordination. Database locks are held briefly (milliseconds per transaction), making contention unlikely for typical workflow submission rates. In the rare case of write conflicts, Sprocket will automatically retry the transaction with exponential backoff.</p>
<h2 id="directory-structure"><a class="header" href="#directory-structure">Directory Structure</a></h2>
<h3 id="output-directory-layout"><a class="header" href="#output-directory-layout">Output Directory Layout</a></h3>
<p>The output directory is the fundamental unit of organization containing all workflow executions, metadata, and indexes:</p>
<pre><code>./out/
‚îú‚îÄ database.db                        # SQLite provenance database
‚îú‚îÄ database.db-shm                    # SQLite shared memory (WAL mode)
‚îú‚îÄ database.db-wal                    # SQLite write-ahead log (WAL mode)
‚îú‚îÄ runs/                              # Workflow execution directories
‚îÇ  ‚îî‚îÄ &lt;workflow_name&gt;/                # Workflow-specific directory
‚îÇ     ‚îú‚îÄ &lt;timestamp&gt;/                 # Individual run (YYYY-MM-DD_HHMMSSffffff)
‚îÇ     ‚îÇ  ‚îî‚îÄ calls/                    # Task execution directories
‚îÇ     ‚îÇ     ‚îî‚îÄ &lt;task_call_id&gt;/        # Task identifier (e.g., "hello-0")
‚îÇ     ‚îÇ        ‚îú‚îÄ attempts/           # Retry attempts directory
‚îÇ     ‚îÇ        ‚îÇ  ‚îî‚îÄ &lt;attempt_number&gt;/  # Attempt number (0, 1, 2, ...)
‚îÇ     ‚îÇ        ‚îÇ     ‚îú‚îÄ command       # Executed shell script
‚îÇ     ‚îÇ        ‚îÇ     ‚îú‚îÄ stdout        # Task standard output
‚îÇ     ‚îÇ        ‚îÇ     ‚îú‚îÄ stderr        # Task standard error
‚îÇ     ‚îÇ        ‚îÇ     ‚îî‚îÄ work/         # Task working directory
‚îÇ     ‚îÇ        ‚îÇ        ‚îî‚îÄ &lt;output_files&gt;  # Task-generated output files
‚îÇ     ‚îÇ        ‚îî‚îÄ tmp/                # Temporary localization files
‚îÇ     ‚îî‚îÄ _latest -&gt; &lt;timestamp&gt;/      # Symlink to most recent run (Unix only)
‚îî‚îÄ index/                             # Optional symlinked organization
   ‚îî‚îÄ &lt;user_defined_path&gt;/            # Created via --index-on flag
      ‚îî‚îÄ &lt;symlinks_to_outputs&gt;        # Symlinks to files in runs/
</code></pre>
<h3 id="directory-behaviors"><a class="header" href="#directory-behaviors">Directory Behaviors</a></h3>
<ul>
<li>On first <code>sprocket run</code> or <code>sprocket server</code> invocation, <code>./out/</code> is created if missing, <code>database.db</code> is initialized with schema migrations. If invoked via <code>sprocket run</code>, the <code>runs/&lt;workflow_name&gt;/&lt;timestamp&gt;/</code> directory structure is created for execution.</li>
<li>Output directory location defaults to <code>./out/</code> relative to current working directory, configurable via <code>--out-dir</code> flag, <code>SPROCKET_OUTPUT_DIR</code> environment variable, or <code>~/.config/sprocket/Sprocket.toml</code>.</li>
<li>The entire output directory is relocatable via <code>mv</code>. All paths stored in <code>database.db</code> are relative to the database file, enabling portability when the output directory is moved.</li>
</ul>
<h2 id="index-functionality"><a class="header" href="#index-functionality">Index Functionality</a></h2>
<p>The index provides user-defined logical organization of outputs on top of the execution-oriented runs directory structure. On Windows, creating symlinks requires administrator privileges or <a href="https://blogs.windows.com/windowsdeveloper/2016/12/02/symlinks-windows-10/">Developer Mode</a> (Windows 10 Insiders build 14972 or later). Windows 11 allows unprivileged symlink creation without Developer Mode. If Sprocket cannot create symlinks due to insufficient permissions, index creation will fail with an error instructing the user to run with administrator privileges or enable Developer Mode. Index symlinks are explicitly requested by the user via <code>--index-on</code> and their failure prevents the expected organization from being created. The <code>_latest</code> symlink (pointing to the most recent run) will be attempted on all platforms but will emit a debug-level log message on failure rather than an error, allowing workflows to complete successfully even when symlink creation is not possible. The <code>_latest</code> symlink is a convenience feature and its absence doesn‚Äôt prevent workflow completion or results access.</p>
<h3 id="index-creation"><a class="header" href="#index-creation">Index Creation</a></h3>
<p>Users create indexes via the <code>--index-on</code> flag:</p>
<pre><code class="language-bash">sprocket run yak_shaving.wdl -i inputs.json \
  --index-on "YakProject/2025/Fluffy"
</code></pre>
<p>This produces:</p>
<pre><code>./out/
‚îú‚îÄ runs/
‚îÇ  ‚îî‚îÄ yak_shaving/
‚îÇ     ‚îî‚îÄ 2025-11-07_143022123456/
‚îÇ        ‚îî‚îÄ calls/
‚îÇ           ‚îî‚îÄ trim_and_style/
‚îÇ              ‚îî‚îÄ attempts/
‚îÇ                 ‚îî‚îÄ 0/
‚îÇ                    ‚îî‚îÄ work/
‚îÇ                       ‚îú‚îÄ final_photo.jpg
‚îÇ                       ‚îî‚îÄ grooming_report/
‚îÇ                          ‚îú‚îÄ satisfaction.html
‚îÇ                          ‚îî‚îÄ style_metrics.txt
‚îî‚îÄ index/
   ‚îî‚îÄ YakProject/
      ‚îî‚îÄ 2025/
         ‚îî‚îÄ Fluffy/
            ‚îú‚îÄ outputs.json         # Complete workflow outputs (all types)
            ‚îú‚îÄ final_photo.jpg -&gt; ../../../../runs/yak_shaving/2025-11-07_143022123456/calls/trim_and_style/attempts/0/work/final_photo.jpg
            ‚îî‚îÄ grooming_report -&gt; ../../../../runs/yak_shaving/2025-11-07_143022123456/calls/trim_and_style/attempts/0/work/grooming_report
</code></pre>
<h3 id="symlinking-behavior"><a class="header" href="#symlinking-behavior">Symlinking Behavior</a></h3>
<p>All workflow output files and directories (as declared in the WDL workflow‚Äôs <code>output</code> section) are symlinked into the index path. An <code>outputs.json</code> file containing the complete workflow outputs (including primitive types like strings, integers, and booleans that cannot be symlinked) is also written to the index path alongside the symlinks. Task-internal files not declared as workflow outputs are not indexed. File outputs are symlinked directly to the file, while directory outputs have the directory itself symlinked rather than individual files within it.</p>
<p>When a workflow is re-run with the same <code>--index-on</code> path, the existing <code>outputs.json</code> is replaced and existing symlinks at that path are removed. New symlinks pointing to the latest run outputs are created. The index always reflects the most recent successful run for a given index path, with the complete history preserved in the <code>index_log</code> database table.</p>
<p>Symlinks use relative paths (e.g., <code>../../../../runs/...</code>), allowing the entire <code>./out/</code> directory to be moved while preserving index functionality. If the index directory is accidentally deleted or needs to be reconstructed, users can rebuild the index from the database history:</p>
<pre><code class="language-bash">sprocket index rebuild --out-dir ./out
</code></pre>
<p>This command queries the <code>index_log</code> table for the most recent entry for each distinct index path and recreates the corresponding symlinks and <code>outputs.json</code> files, restoring the index to its last known state.</p>
<h2 id="cli-workflow"><a class="header" href="#cli-workflow">CLI Workflow</a></h2>
<h3 id="execution-flow"><a class="header" href="#execution-flow">Execution Flow</a></h3>
<pre><code class="language-mermaid">sequenceDiagram
    actor User
    participant CLI as sprocket run
    participant DB as database.db
    participant FS as Filesystem
    participant Engine as Workflow Engine

    User-&gt;&gt;CLI: sprocket run yak_shaving.wdl --index-on "YakProject/2025/Fluffy"
    CLI-&gt;&gt;FS: Check for ./out/, create if doesn't exist
    CLI-&gt;&gt;FS: Check for database.db, create if doesn't exist
    CLI-&gt;&gt;DB: Connect to database.db (WAL mode)
    CLI-&gt;&gt;DB: INSERT invocation (submission_method=cli, created_by=$USER)
    CLI-&gt;&gt;DB: INSERT workflow (status=pending)
    CLI-&gt;&gt;FS: Create runs/yak_shaving/2025-11-07_143022/
    CLI-&gt;&gt;DB: UPDATE workflow (status=running)
    CLI-&gt;&gt;Engine: Execute workflow
    Engine-&gt;&gt;FS: Write execution files (stdout, stderr, outputs)
    Engine--&gt;&gt;CLI: Execution complete
    CLI-&gt;&gt;DB: UPDATE workflow (status=completed, outputs=...)
    CLI-&gt;&gt;FS: Create index symlinks at YakProject/2025/Fluffy/
    CLI--&gt;&gt;User: Workflow complete
</code></pre>
<p>The output directory location is resolved from command-line flags (<code>--out-dir</code>), environment variables (<code>SPROCKET_output_dir</code>), configuration file settings, or the default <code>./out/</code>. Each CLI invocation generates a UUID for the run and creates its own invocation record with <code>created_by</code> populated from the <code>$USER</code> environment variable. Using the actor-based architecture described in the Architecture Overview, the workflow manager actor manages a single workflow execution task for the duration of the CLI command. When a workflow completes successfully, database records are updated with outputs and completion timestamp, and index symlinks are created if <code>--index-on</code> was specified. Failed workflows update the database with error information but do not create index symlinks, as there are no valid outputs to index.</p>
<h2 id="server-mode"><a class="header" href="#server-mode">Server Mode</a></h2>
<p>The server provides an HTTP API for submitting workflows and querying run history.</p>
<h3 id="server-configuration"><a class="header" href="#server-configuration">Server Configuration</a></h3>
<h3 id="execution-flow-1"><a class="header" href="#execution-flow-1">Execution Flow</a></h3>
<pre><code class="language-mermaid">sequenceDiagram
    actor User
    participant Server as sprocket server
    participant DB as database.db
    participant FS as Filesystem
    participant Engine as Workflow Engine

    Note over User,DB: Phase 1: Server Startup
    User-&gt;&gt;Server: sprocket server --out-dir ./out
    Server-&gt;&gt;FS: Check for ./out/, create if doesn't exist
    Server-&gt;&gt;FS: Check for database.db, create if doesn't exist
    Server-&gt;&gt;DB: Connect to database.db (WAL mode)
    Server-&gt;&gt;DB: INSERT invocation (submission_method=http, created_by=$USER)
    Server-&gt;&gt;Server: Start HTTP server
    Server--&gt;&gt;User: Server running

    Note over User,FS: Phase 2: Workflow Submission &amp; Execution
    User-&gt;&gt;Server: POST /api/workflows {source, inputs, index_path}
    Server-&gt;&gt;DB: INSERT workflow (status=pending, invocation_id=...)
    Server--&gt;&gt;User: 201 Created {id, status}
    Server-&gt;&gt;FS: Create runs/yak_shaving/2025-11-07_143022/
    Server-&gt;&gt;DB: UPDATE workflow (status=running)
    Server-&gt;&gt;Engine: Execute workflow (async)
    Engine-&gt;&gt;FS: Write execution files (stdout, stderr, outputs)
    User-&gt;&gt;Server: GET /api/workflows/{id}
    Server-&gt;&gt;DB: SELECT workflow WHERE id=...
    Server--&gt;&gt;User: 200 OK {status: running, ...}
    Engine--&gt;&gt;Server: Execution complete
    Server-&gt;&gt;DB: UPDATE workflow (status=completed, outputs=...)
    Server-&gt;&gt;FS: Create index symlinks if index_path provided

    Note over User,FS: Phase 3: Viewing Completed Results
    User-&gt;&gt;Server: GET /api/workflows/{id}
    Server-&gt;&gt;DB: SELECT workflow WHERE id=...
    Server--&gt;&gt;User: 200 OK {status: completed, outputs: {...}}
</code></pre>
<p>The server resolves the output directory location from command-line flags, environment variables, configuration file settings, or the default <code>./out/</code>. On startup, it initializes the output directory and database if not present, or connects to an existing database in WAL mode for concurrent access. A single invocation record is created at server startup with <code>submission_method = 'http'</code>, and all workflows submitted to that server instance share this invocation. Using the actor-based architecture described in the Architecture Overview, the workflow manager actor spawns independent Tokio tasks for each submitted workflow, enabling concurrent execution without blocking API requests. An optional concurrency limit may be configured to control maximum parallel workflow executions based on available system resources.</p>
<h3 id="rest-api-endpoints"><a class="header" href="#rest-api-endpoints">REST API Endpoints</a></h3>
<h4 id="post-apiworkflows"><a class="header" href="#post-apiworkflows"><code>POST /api/workflows</code></a></h4>
<p>Submit a new workflow for execution.</p>
<p><strong>Request:</strong></p>
<pre><code class="language-json">{
  "source": "https://github.com/user/repo/yak_shaving.wdl",
  "inputs": {
    "yak_name": "Fluffy",
    "style": "mohawk"
  },
  "index_path": "YakProject/2025/Fluffy"  // Optional
}
</code></pre>
<p><strong>Response:</strong> <code>201 Created</code></p>
<pre><code class="language-json">{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "created_at": "2025-11-07T14:30:22Z"
}
</code></pre>
<h4 id="get-apiworkflows"><a class="header" href="#get-apiworkflows"><code>GET /api/workflows</code></a></h4>
<p>Query workflow executions with optional filters.</p>
<p><strong>Request:</strong></p>
<pre><code>GET /api/workflows?status=running&amp;name=yak_shaving&amp;limit=50
</code></pre>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "workflows": [
    {
      "id": "550e8400-e29b-41d4-a716-446655440000",
      "name": "yak_shaving",
      "status": "running",
      "invocation_id": "660e8400-e29b-41d4-a716-446655440001",
      "created_at": "2025-11-07T14:30:22Z",
      "started_at": "2025-11-07T14:30:23Z"
    }
  ]
}
</code></pre>
<h4 id="get-apiworkflowsid"><a class="header" href="#get-apiworkflowsid"><code>GET /api/workflows/{id}</code></a></h4>
<p>Retrieve detailed information about a specific workflow execution.</p>
<p><strong>Request:</strong></p>
<pre><code>GET /api/workflows/550e8400-e29b-41d4-a716-446655440000
</code></pre>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "name": "yak_shaving",
  "source": "https://github.com/user/repo/yak_shaving.wdl",
  "status": "completed",
  "invocation_id": "660e8400-e29b-41d4-a716-446655440001",
  "inputs": {
    "yak_name": "Fluffy",
    "style": "mohawk"
  },
  "outputs": {
    "final_photo": "runs/yak_shaving/2025-11-07_143022123456/calls/trim_and_style/attempts/0/work/final_photo.jpg"
  },
  "execution_dir": "runs/yak_shaving/2025-11-07_143022123456",
  "created_at": "2025-11-07T14:30:22Z",
  "started_at": "2025-11-07T14:30:23Z",
  "completed_at": "2025-11-07T14:45:10Z"
}
</code></pre>
<p>CLI and server operate simultaneously on the same output directory. A workflow submitted via <code>sprocket run</code> appears in <code>GET /api/workflows</code> queries as soon as the database transaction commits. All workflows share the same database regardless of submission method.</p>
<h2 id="rationale-and-alternatives-1"><a class="header" href="#rationale-and-alternatives-1">Rationale and Alternatives</a></h2>
<h3 id="why-sqlite"><a class="header" href="#why-sqlite">Why SQLite?</a></h3>
<p>The choice of SQLite as the provenance database is fundamental to achieving progressive disclosure. The database must support several key scenarios.</p>
<ul>
<li>Users running <code>sprocket run workflow.wdl</code> for the first time should get working provenance tracking without installing, configuring, or even thinking about a database.</li>
<li>Multiple <code>sprocket run</code> processes must safely write to the database simultaneously, and a running <code>sprocket server</code> must be able to query historical runs created by CLI while also accepting new submissions.</li>
<li>Moving the output directory with <code>mv</code> or <code>rsync</code> should preserve all functionality without database reconfiguration or path updates.</li>
<li>Database operations should add negligible overhead to workflow execution, which typically takes minutes to hours.</li>
</ul>
<p>SQLite excels at meeting these requirements because it is embedded directly in the Sprocket binary. The database file is created automatically on first use with no user action required‚Äîthere are no connection strings to configure, no server processes to start, no permissions to manage. The database is a single file (<code>database.db</code>) within the output directory, so moving the directory preserves the database without any connection reconfiguration. This filesystem-based portability is essential to the self-contained output directory design.</p>
<p>SQLite‚Äôs Write-Ahead Logging (WAL) mode enables multiple concurrent readers with a single writer at a time, where reads never block writes and vice versa. This is sufficient for workflow submission patterns, where writes are infrequent‚Äîone per workflow execution, taking milliseconds‚Äîcompared to workflow execution time of minutes to hours. Even in high-throughput environments, workflow submission rates rarely exceed one per second, while SQLite WAL mode handles <a href="https://victoria.dev/posts/sqlite-in-production-with-wal/">400 write transactions per second and thousands of reads on modest hardware</a> and <a href="https://highperformancesqlite.com/watch/wal-vs-journal-benchmarks">3,600 writes per second with 70,000 reads per second in standard benchmarks</a>. Operational simplicity is another major advantage‚Äîthere‚Äôs no backup strategy beyond filesystem backups, no version compatibility issues between client and server, and no network latency or connection pooling concerns.</p>
<p>While not a major driving factor, SQLite‚Äôs ubiquity should be considered for users who want to build custom tooling on top of the provenance database. Native language bindings exist for virtually every programming language and platform, and the file format is stable and well-documented. Users can query the database directly using standard SQLite clients, build custom analysis scripts in their preferred language, or integrate the database into dashboards without needing Sprocket-specific APIs.</p>
<h3 id="future-work-postgresql-support"><a class="header" href="#future-work-postgresql-support">Future Work: PostgreSQL Support</a></h3>
<p>PostgreSQL support is planned as future work to enable enterprise-scale deployments. PostgreSQL offers better concurrent write performance through <a href="https://www.postgresql.org/docs/current/mvcc-intro.html">MVCC (Multi-Version Concurrency Control)</a>, remote database access over the network, more sophisticated query optimization for complex queries, and proven scalability to millions of records. These capabilities become valuable when organizations need to run multiple Sprocket server instances sharing a central database, or when workflow submission rates exceed SQLite‚Äôs single-writer throughput.</p>
<p>However, PostgreSQL requires separate server installation and configuration, violating the zero-configuration principle that makes Sprocket accessible to new users. Users must provision database infrastructure, manage connection strings with host, port, and credentials, and coordinate backups independently of the output directory. Network dependencies introduce new failure modes, and version compatibility between PostgreSQL server and Sprocket client becomes an operational concern. For the majority of use cases‚Äîsingle servers with infrequent writes and simple queries‚ÄîPostgreSQL‚Äôs complexity outweighs its benefits.</p>
<p>The implementation strategy will introduce a database abstraction layer that keeps SQLite as the default while allowing users to opt into PostgreSQL via configuration. An automatic migration tool will transfer existing SQLite databases to PostgreSQL, preserving all workflow execution history and index logs. The filesystem-based output directory structure (<code>runs/</code> and <code>index/</code>) will remain unchanged regardless of database backend, ensuring that users can migrate from SQLite to PostgreSQL without disrupting their existing workflows or file organization. This approach maintains progressive disclosure: users start with zero-configuration SQLite and migrate to PostgreSQL only when scale demands it.</p>
<h3 id="alternative-embedded-key-value-stores-rocksdb-lmdb"><a class="header" href="#alternative-embedded-key-value-stores-rocksdb-lmdb">Alternative: Embedded Key-Value Stores (RocksDB, LMDB)</a></h3>
<p>Embedded key-value stores like RocksDB or LMDB are appealing because they share SQLite‚Äôs zero-configuration, embedded nature while offering exceptional performance. RocksDB appears to be the strongest option, achieving <a href="https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks">~86,000 writes per second for random overwrites and ~137,000-189,000 reads per second</a>, while LMDB is <a href="https://mozilla.github.io/firefox-browser-architecture/text/0017-lmdb-vs-leveldb.html">optimized for read performance with competitive write throughput</a>. Though RocksDB bulk insertion showed around ~1 million writes per second in benchmarks, workflow metadata tracking performs multiple individual transactional writes per workflow (invocation inserts, workflow record inserts and updates, index log entries), making the random overwrite performance more applicable to this use case. These systems are purpose-built for high-throughput append-heavy workloads and would handle workflow metadata operations effortlessly. However, they were rejected for this initial implementation because they complicate data access patterns without providing necessary performance benefits.</p>
<p>The performance advantage of key-value stores is irrelevant for this use case. Workflow execution takes minutes to hours, while database writes complete in milliseconds regardless of the underlying storage engine. Even at high submission rates of one workflow per second, SQLite‚Äôs throughput of hundreds to thousands of transactions per second provides ample headroom. The bottleneck is never the database‚Äîit‚Äôs the workflow execution itself.</p>
<p>While key-value stores do lack the familiar SQL interface that users expect when querying execution history, this is a secondary concern compared to the implementation and maintenance complexity they introduce. If future requirements reveal that SQLite‚Äôs single-writer limitation becomes a genuine bottleneck‚Äîwhich would require sustained submission rates exceeding hundreds per second‚Äîkey-value stores would be reconsidered as a filesystem-based solution. However, such rates are unrealistic for workflow engines, and if they materialize, PostgreSQL with its MVCC support would likely be a better fit for the access patterns involved.</p>
<h3 id="alternative-filesystem-only-no-database"><a class="header" href="#alternative-filesystem-only-no-database">Alternative: Filesystem Only (No Database)</a></h3>
<p>A filesystem-only approach without a database was rejected because it creates filesystem stress and operational problems, particularly in HPC environments where Sprocket is likely to be deployed. This approach would store metadata in JSON files alongside workflow execution directories (e.g., <code>runs/&lt;workflow&gt;/&lt;timestamp&gt;/metadata.json</code>). While this has zero dependencies beyond the filesystem, is simple to implement, and is naturally portable, it introduces several critical issues.</p>
<p>Storing metadata as individual files creates <a href="https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/hpc-storage/best-practices">inode exhaustion problems common in HPC environments</a>, where each workflow execution would generate multiple small metadata files. HPC filesystems often have strict inode quotas, and large numbers of small files create bottlenecks on metadata servers that degrade performance for all users of the shared filesystem. Queries require walking directory trees and parsing potentially thousands of JSON files, putting additional stress on filesystem metadata operations‚Äîthe exact workload that HPC storage administrators actively discourage. There‚Äôs no standardized query interface and no efficient indexing for common queries like ‚Äúshow all running workflows.‚Äù Monitoring tools must implement custom file-walking logic, and race conditions emerge when updating metadata from multiple processes without database-level transaction guarantees.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="drafts"><a class="header" href="#drafts">Drafts</a></h1>
<p>The following are <em>candidate</em> RFCs that are being rendered for easy review. They are <em>not</em> accepted St. Jude Rust Labs RFCs. For more information please see <a href="https://github.com/stjude-rust-labs/rfcs/pulls">the associated pull request</a>.</p>
<ul>
<li><a href="https://stjude-rust-labs.github.io/rfcs/branches/0003-hints-registry">0003-hints-registry</a></li>
<li><a href="https://stjude-rust-labs.github.io/rfcs/branches/0004-task-classes">0004-task-classes</a></li>
<li><a href="https://stjude-rust-labs.github.io/rfcs/branches/HEAD">HEAD</a></li>
<li><a href="https://stjude-rust-labs.github.io/rfcs/branches/fix/ci">fix/ci</a></li>
<li><a href="https://stjude-rust-labs.github.io/rfcs/branches/provenance">provenance</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
